{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dask Dataframes on NYC Taxi Data\n",
    "================================\n",
    "\n",
    "<img src=\"http://pandas.pydata.org/_static/pandas_logo.png\"\n",
    "     align=\"left\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Pandas logo\">\n",
    "     <img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\">\n",
    "\n",
    "In this section we will learn how to ...\n",
    "\n",
    "-  use Dask Dataframe to scale Pandas workloads\n",
    "-  call `.compute` and `.persist` to trigger computation\n",
    "-  start and scale a Dask cluster on Kubernetes\n",
    "-  interpret dashboard plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have several CSV files in cloud storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gcsfs import GCSFileSystem\n",
    "gcs = GCSFileSystem()\n",
    "\n",
    "sorted(gcs.glob('anaconda-public-data/nyc-taxi/csv/2015/yellow_*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a subset with Pandas\n",
    "\n",
    "It's too big to fit in memory on a single machine, so we pull out the first million rows to get a first impression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with gcs.open('anaconda-public-data/nyc-taxi/csv/2015/yellow_tripdata_2015-01.csv') as f:\n",
    "    df = pd.read_csv(f, nrows=1000000, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate the subset as normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)  # How many rides in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count.sum()  # How many passengers total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count.value_counts()  # What was the distribution of passengers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df.passenger_count).trip_distance.mean()  # What was the average trip distance, grouped by passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tip Fraction, grouped by hour-of-day\n",
    "df2 = df[(df.tip_amount > 0) & (df.fare_amount > 0)]\n",
    "df2['tip_fraction'] = df2.tip_amount / df2.fare_amount\n",
    "hour = df2.groupby(df2.tpep_pickup_datetime.dt.hour).tip_fraction.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "hour.plot(figsize=(10, 6), title='Tip Fraction by Hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a Dask Cluster\n",
    "\n",
    "Your notebook is conveniently attached to a Kubernetes cluster, so you can start a Dask cluster using the [dask-kubernetes](https://kubernetes.dask.org/en/latest/) project.\n",
    "\n",
    "For more information on deploying Dask on different cluster technology see [Dask's deployment documentation](https://docs.dask.org/en/latest/setup.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_kubernetes import KubeCluster\n",
    "cluster = KubeCluster(n_workers=20)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dask dataframe around all of the data\n",
    "\n",
    "Before we loaded only a subset of one CSV file.  Now lets use Dask dataframe to read all of the files.\n",
    "\n",
    "For more information you can read [Dask's documentation for creating dataframes](http://docs.dask.org/en/latest/dataframe-create.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv('gcs://anaconda-public-data/nyc-taxi/csv/2015/yellow_*.csv', \n",
    "                 parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask dataframes look like Pandas dataframes, and support most of the common Pandas methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.passenger_count.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate laziness\n",
    "\n",
    "Note that this did not yet load the data into memory.  Dask dataframes are *lazy* by default, so they only evaluate when we tell them to.\n",
    "\n",
    "There are two ways to trigger computation:\n",
    "\n",
    "-  `result = result.compute()`: triggers computation and stores the result into local memory as a Pandas object.  You should use this with *small* results that will fit into memory.\n",
    "-  `result = result.persist()`: triggers computation and stores the result into distributed memory, returning another Dask dataframe object.  You should use this with *large* results that you want to stage in distributed memory for repeated computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Task*: determine how many passengers there were in 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Task*: determine the distribution of passengers\n",
    "\n",
    "How many rides were there with one passenger, two passengers, and so on?\n",
    "\n",
    "How does this compare to our sample from before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Task*: determine the average trip distance, grouped by passenger count\n",
    "\n",
    "How does this compare to our sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Task*: What did our workers spend their time doing?\n",
    "\n",
    "To answer this question look at the Task Stream dashboard plot.  It will tell you the activity on each core of your cluster (y-axis) over time (x-axis).  You can hover over each rectangle of this plot to determine what kind of task it was.  What kinds of tasks are most common and take up the most time?\n",
    "\n",
    "*TODO*: Add image of task stream plot\n",
    "\n",
    "*Extra*: if you're ahead of the group you might also want to look at the Profile dashboard plot.  You can access this by selecting the orange Dask icon on the left side of your JupyterLab page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist data in memory\n",
    "\n",
    "Each time we call compute we re-evaluate the entire computation, starting with downloading data from cloud storage, reading that CSV data into Pandas dataframes, and then doing the normal Pandas groupbys and such that we care about.\n",
    "\n",
    "It is common to do some initial data preparation work (loading and parsing data) and then want to persist that data into distributed memory for faster access.  You can do this with the `.persist` method.\n",
    "\n",
    "    result = result.persist()\n",
    "    \n",
    "Do this now and then repeat your computations from before.  How does this affect computation time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Task*: Persist your dataframe into memory, then repeat the earlier computations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the distribution of passengers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine how many passengers there were in 2015\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the average trip distance, grouped by passenger count\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
